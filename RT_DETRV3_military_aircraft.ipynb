{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv12 Implementation for Military Aircraft Detection Dataset\n",
    "\n",
    "This notebook implements YOLOv12 for the Military Aircraft Detection Dataset available on Kaggle. The implementation includes data preprocessing, model architecture, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics pandas matplotlib seaborn scikit-learn opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define dataset paths\n",
    "DATASET_PATH = '../input/militaryaircraftdetectiondataset/dataset'\n",
    "CROP_PATH = '../input/militaryaircraftdetectiondataset/crop'\n",
    "ANNOTATED_SAMPLES_PATH = '../input/militaryaircraftdetectiondataset/annotated_samples'\n",
    "\n",
    "# Check if paths exist\n",
    "print(f\"Dataset path exists: {os.path.exists(DATASET_PATH)}\")\n",
    "print(f\"Crop path exists: {os.path.exists(CROP_PATH)}\")\n",
    "print(f\"Annotated samples path exists: {os.path.exists(ANNOTATED_SAMPLES_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List image and annotation files\n",
    "image_files = glob.glob(os.path.join(DATASET_PATH, '*.jpg'))\n",
    "annotation_files = glob.glob(os.path.join(DATASET_PATH, '*.csv'))\n",
    "\n",
    "print(f\"Number of images: {len(image_files)}\")\n",
    "print(f\"Number of annotation files: {len(annotation_files)}\")\n",
    "\n",
    "# Check class distribution\n",
    "all_classes = []\n",
    "for csv_file in tqdm(annotation_files[:100]):  # Sample first 100 for quick analysis\n",
    "    df = pd.read_csv(csv_file)\n",
    "    all_classes.extend(df['class'].tolist())\n",
    "\n",
    "class_counts = pd.Series(all_classes).value_counts()\n",
    "print(f\"Number of unique classes in sample: {len(class_counts)}\")\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(x=class_counts.index[:20], y=class_counts.values[:20])\n",
    "plt.title('Top 20 Aircraft Classes Distribution')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize some sample images with annotations\n",
    "def visualize_sample(image_path, annotation_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    df = pd.read_csv(annotation_path)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "        class_name = row['class']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, \n",
    "                                         fill=False, edgecolor='red', linewidth=2))\n",
    "        plt.text(xmin, ymin-10, class_name, color='red', fontsize=12, \n",
    "                 bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.title(f\"Image with annotations: {os.path.basename(image_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples\n",
    "for i in range(min(3, len(image_files))):\n",
    "    img_path = image_files[i]\n",
    "    ann_path = os.path.join(DATASET_PATH, os.path.basename(img_path).replace('.jpg', '.csv'))\n",
    "    if os.path.exists(ann_path):\n",
    "        visualize_sample(img_path, ann_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for YOLOv12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create class mapping\n",
    "def create_class_mapping():\n",
    "    # Get all unique classes from annotation files\n",
    "    all_classes = set()\n",
    "    for csv_file in tqdm(annotation_files):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if 'class' in df.columns:\n",
    "                all_classes.update(df['class'].unique())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "    \n",
    "    # Create class mapping\n",
    "    class_list = sorted(list(all_classes))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(class_list)}\n",
    "    \n",
    "    # Save class mapping\n",
    "    with open('classes.txt', 'w') as f:\n",
    "        for cls in class_list:\n",
    "            f.write(f\"{cls}\\n\")\n",
    "    \n",
    "    return class_list, class_to_idx\n",
    "\n",
    "# Convert annotations to YOLO format\n",
    "def convert_to_yolo_format(class_to_idx):\n",
    "    os.makedirs('yolo_dataset/images/train', exist_ok=True)\n",
    "    os.makedirs('yolo_dataset/images/val', exist_ok=True)\n",
    "    os.makedirs('yolo_dataset/labels/train', exist_ok=True)\n",
    "    os.makedirs('yolo_dataset/labels/val', exist_ok=True)\n",
    "    \n",
    "    # Split dataset into train and validation\n",
    "    all_files = [os.path.basename(f).replace('.jpg', '') for f in image_files]\n",
    "    train_files, val_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_files)}\")\n",
    "    print(f\"Validation samples: {len(val_files)}\")\n",
    "    \n",
    "    # Process training files\n",
    "    for file_id in tqdm(train_files):\n",
    "        img_path = os.path.join(DATASET_PATH, f\"{file_id}.jpg\")\n",
    "        csv_path = os.path.join(DATASET_PATH, f\"{file_id}.csv\")\n",
    "        \n",
    "        if not os.path.exists(img_path) or not os.path.exists(csv_path):\n",
    "            continue\n",
    "        \n",
    "        # Copy image to train folder\n",
    "        os.system(f\"cp '{img_path}' 'yolo_dataset/images/train/{file_id}.jpg'\")\n",
    "        \n",
    "        # Convert annotations to YOLO format\n",
    "        df = pd.read_csv(csv_path)\n",
    "        img_width = df['width'].iloc[0]\n",
    "        img_height = df['height'].iloc[0]\n",
    "        \n",
    "        with open(f\"yolo_dataset/labels/train/{file_id}.txt\", 'w') as f:\n",
    "            for _, row in df.iterrows():\n",
    "                class_name = row['class']\n",
    "                if class_name not in class_to_idx:\n",
    "                    continue\n",
    "                    \n",
    "                class_id = class_to_idx[class_name]\n",
    "                x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "                \n",
    "                # Convert to YOLO format (center_x, center_y, width, height) normalized\n",
    "                x_center = ((x_min + x_max) / 2) / img_width\n",
    "                y_center = ((y_min + y_max) / 2) / img_height\n",
    "                width = (x_max - x_min) / img_width\n",
    "                height = (y_max - y_min) / img_height\n",
    "                \n",
    "                f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "    \n",
    "    # Process validation files\n",
    "    for file_id in tqdm(val_files):\n",
    "        img_path = os.path.join(DATASET_PATH, f\"{file_id}.jpg\")\n",
    "        csv_path = os.path.join(DATASET_PATH, f\"{file_id}.csv\")\n",
    "        \n",
    "        if not os.path.exists(img_path) or not os.path.exists(csv_path):\n",
    "            continue\n",
    "        \n",
    "        # Copy image to val folder\n",
    "        os.system(f\"cp '{img_path}' 'yolo_dataset/images/val/{file_id}.jpg'\")\n",
    "        \n",
    "        # Convert annotations to YOLO format\n",
    "        df = pd.read_csv(csv_path)\n",
    "        img_width = df['width'].iloc[0]\n",
    "        img_height = df['height'].iloc[0]\n",
    "        \n",
    "        with open(f\"yolo_dataset/labels/val/{file_id}.txt\", 'w') as f:\n",
    "            for _, row in df.iterrows():\n",
    "                class_name = row['class']\n",
    "                if class_name not in class_to_idx:\n",
    "                    continue\n",
    "                    \n",
    "                class_id = class_to_idx[class_name]\n",
    "                x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "                \n",
    "                # Convert to YOLO format (center_x, center_y, width, height) normalized\n",
    "                x_center = ((x_min + x_max) / 2) / img_width\n",
    "                y_center = ((y_min + y_max) / 2) / img_height\n",
    "                width = (x_max - x_min) / img_width\n",
    "                height = (y_max - y_min) / img_height\n",
    "                \n",
    "                f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "    \n",
    "    # Create dataset YAML file\n",
    "    with open('yolo_dataset/dataset.yaml', 'w') as f:\n",
    "        f.write(f\"train: {os.path.abspath('yolo_dataset/images/train')}\\n\")\n",
    "        f.write(f\"val: {os.path.abspath('yolo_dataset/images/val')}\\n\")\n",
    "        f.write(f\"nc: {len(class_to_idx)}\\n\")\n",
    "        f.write(f\"names: {list(class_to_idx.keys())}\\n\")\n",
    "\n",
    "# Run preprocessing\n",
    "print(\"Creating class mapping...\")\n",
    "class_list, class_to_idx = create_class_mapping()\n",
    "print(f\"Found {len(class_list)} unique classes\")\n",
    "\n",
    "print(\"\\nConverting annotations to YOLO format...\")\n",
    "convert_to_yolo_format(class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YOLOv12 Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOLOv12 Architecture Components\n",
    "\n",
    "class ConvBNSiLU(nn.Module):\n",
    "    \"\"\"Convolution + BatchNorm + SiLU activation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=None, groups=1, bias=False):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class ELAN(nn.Module):\n",
    "    \"\"\"Enhanced Layer Aggregation Network for YOLOv12\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, expansion=0.5):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.cv1 = ConvBNSiLU(in_channels, hidden_channels, 1, 1)\n",
    "        self.cv2 = ConvBNSiLU(hidden_channels, hidden_channels, 3, 1)\n",
    "        self.cv3 = ConvBNSiLU(hidden_channels, hidden_channels, 3, 1)\n",
    "        self.cv4 = ConvBNSiLU(hidden_channels, hidden_channels, 3, 1)\n",
    "        self.cv5 = ConvBNSiLU(hidden_channels, hidden_channels, 3, 1)\n",
    "        self.cv6 = ConvBNSiLU(hidden_channels * 4, out_channels, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        x2 = self.cv2(x)\n",
    "        x3 = self.cv3(x2)\n",
    "        x4 = self.cv4(x3)\n",
    "        x5 = self.cv5(x4)\n",
    "        out = self.cv6(torch.cat([x2, x3, x4, x5], dim=1))\n",
    "        return out\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling - Fast\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.cv1 = ConvBNSiLU(in_channels, hidden_channels, 1, 1)\n",
    "        self.cv2 = ConvBNSiLU(hidden_channels * 4, out_channels, 1, 1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.maxpool(x)\n",
    "        y2 = self.maxpool(y1)\n",
    "        y3 = self.maxpool(y2)\n",
    "        out = self.cv2(torch.cat([x, y1, y2, y3], dim=1))\n",
    "        return out\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    \"\"\"Downsample module\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBNSiLU(in_channels, out_channels, 3, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class CARAFE(nn.Module):\n",
    "    \"\"\"Content-Aware ReAssembly of FEatures for upsampling\"\"\"\n",
    "    def __init__(self, in_channels, scale_factor=2, up_kernel=5):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.up_kernel = up_kernel\n",
    "        \n",
    "        self.kernel_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, scale_factor * scale_factor * up_kernel * up_kernel, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # Generate kernel weights\n",
    "        kernel_weights = self.kernel_conv(x)\n",
    "        kernel_weights = kernel_weights.view(b, -1, self.up_kernel * self.up_kernel, h, w)\n",
    "        kernel_weights = F.softmax(kernel_weights, dim=2)\n",
    "        kernel_weights = kernel_weights.view(b, -1, h, w)\n",
    "        \n",
    "        # Upsample feature map\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n",
    "        kernel_weights = F.interpolate(kernel_weights, scale_factor=self.scale_factor, mode='nearest')\n",
    "        \n",
    "        # Apply kernel weights\n",
    "        x = F.unfold(x, kernel_size=self.up_kernel, padding=self.up_kernel//2)\n",
    "        x = x.view(b, c, self.up_kernel * self.up_kernel, h * self.scale_factor, w * self.scale_factor)\n",
    "        kernel_weights = kernel_weights.view(b, self.scale_factor * self.scale_factor, self.up_kernel * self.up_kernel, \n",
    "                                           h * self.scale_factor, w * self.scale_factor)\n",
    "        \n",
    "        x = torch.sum(x.unsqueeze(2) * kernel_weights.unsqueeze(1), dim=3)\n",
    "        x = x.view(b, c, h * self.scale_factor, w * self.scale_factor)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class YOLOv12Neck(nn.Module):\n",
    "    \"\"\"YOLOv12 Neck with ELAN and SPPF\"\"\"\n",
    "    def __init__(self, in_channels_list, out_channels_list):\n",
    "        super().__init__()\n",
    "        self.sppf = SPPF(in_channels_list[2], out_channels_list[2])\n",
    "        \n",
    "        # Top-down path\n",
    "        self.carafe1 = CARAFE(out_channels_list[2])\n",
    "        self.elan_td1 = ELAN(out_channels_list[2] + in_channels_list[1], out_channels_list[1])\n",
    "        \n",
    "        self.carafe2 = CARAFE(out_channels_list[1])\n",
    "        self.elan_td2 = ELAN(out_channels_list[1] + in_channels_list[0], out_channels_list[0])\n",
    "        \n",
    "        # Bottom-up path\n",
    "        self.down1 = DownSample(out_channels_list[0], out_channels_list[0])\n",
    "        self.elan_bu1 = ELAN(out_channels_list[0] + out_channels_list[1], out_channels_list[1])\n",
    "        \n",
    "        self.down2 = DownSample(out_channels_list[1], out_channels_list[1])\n",
    "        self.elan_bu2 = ELAN(out_channels_list[1] + out_channels_list[2], out_channels_list[2])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Backbone features\n",
    "        x_small, x_medium, x_large = inputs\n",
    "        \n",
    "        # SPPF on the largest feature map\n",
    "        x_large = self.sppf(x_large)\n",
    "        \n",
    "        # Top-down path\n",
    "        p_large = x_large\n",
    "        p_large_up = self.carafe1(p_large)\n",
    "        p_medium = self.elan_td1(torch.cat([p_large_up, x_medium], dim=1))\n",
    "        \n",
    "        p_medium_up = self.carafe2(p_medium)\n",
    "        p_small = self.elan_td2(torch.cat([p_medium_up, x_small], dim=1))\n",
    "        \n",
    "        # Bottom-up path\n",
    "        p_small_down = self.down1(p_small)\n",
    "        p_medium = self.elan_bu1(torch.cat([p_small_down, p_medium], dim=1))\n",
    "        \n",
    "        p_medium_down = self.down2(p_medium)\n",
    "        p_large = self.elan_bu2(torch.cat([p_medium_down, p_large], dim=1))\n",
    "        \n",
    "        return p_small, p_medium, p_large\n",
    "\n",
    "class YOLOv12Head(nn.Module):\n",
    "    \"\"\"YOLOv12 Detection Head\"\"\"\n",
    "    def __init__(self, in_channels_list, num_classes, num_anchors=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        # Detection heads for different scales\n",
    "        self.det_small = nn.Conv2d(in_channels_list[0], num_anchors * (5 + num_classes), 1)\n",
    "        self.det_medium = nn.Conv2d(in_channels_list[1], num_anchors * (5 + num_classes), 1)\n",
    "        self.det_large = nn.Conv2d(in_channels_list[2], num_anchors * (5 + num_classes), 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        p_small, p_medium, p_large = inputs\n",
    "        \n",
    "        # Apply detection heads\n",
    "        out_small = self.det_small(p_small)\n",
    "        out_medium = self.det_medium(p_medium)\n",
    "        out_large = self.det_large(p_large)\n",
    "        \n",
    "        # Reshape outputs\n",
    "        batch_size = out_small.size(0)\n",
    "        \n",
    "        out_small = out_small.view(batch_size, self.num_anchors, 5 + self.num_classes, out_small.size(2), out_small.size(3))\n",
    "        out_small = out_small.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        out_medium = out_medium.view(batch_size, self.num_anchors, 5 + self.num_classes, out_medium.size(2), out_medium.size(3))\n",
    "        out_medium = out_medium.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        out_large = out_large.view(batch_size, self.num_anchors, 5 + self.num_classes, out_large.size(2), out_large.size(3))\n",
    "        out_large = out_large.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        return out_small, out_medium, out_large\n",
    "\n",
    "class YOLOv12Backbone(nn.Module):\n",
    "    \"\"\"YOLOv12 Backbone with CSPDarknet\"\"\"\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        # Initial convolution\n",
    "        self.conv1 = ConvBNSiLU(in_channels, 32, 3, 1)\n",
    "        \n",
    "        # Downsample and ELAN blocks\n",
    "        self.down1 = DownSample(32, 64)\n",
    "        self.elan1 = ELAN(64, 64)\n",
    "        \n",
    "        self.down2 = DownSample(64, 128)\n",
    "        self.elan2 = ELAN(128, 128)\n",
    "        \n",
    "        self.down3 = DownSample(128, 256)\n",
    "        self.elan3 = ELAN(256, 256)\n",
    "        \n",
    "        self.down4 = DownSample(256, 512)\n",
    "        self.elan4 = ELAN(512, 512)\n",
    "        \n",
    "        self.down5 = DownSample(512, 1024)\n",
    "        self.elan5 = ELAN(1024, 1024)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # Downsample and ELAN blocks\n",
    "        x = self.down1(x)\n",
    "        x = self.elan1(x)\n",
    "        \n",
    "        x = self.down2(x)\n",
    "        x = self.elan2(x)\n",
    "        out_small = x  # Small scale feature map\n",
    "        \n",
    "        x = self.down3(x)\n",
    "        x = self.elan3(x)\n",
    "        out_medium = x  # Medium scale feature map\n",
    "        \n",
    "        x = self.down4(x)\n",
    "        x = self.elan4(x)\n",
    "        \n",
    "        x = self.down5(x)\n",
    "        x = self.elan5(x)\n",
    "        out_large = x  # Large scale feature map\n",
    "        \n",
    "        return out_small, out_medium, out_large\n",
    "\n",
    "class YOLOv12(nn.Module):\n",
    "    \"\"\"Complete YOLOv12 model\"\"\"\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.backbone = YOLOv12Backbone(in_channels)\n",
    "        self.neck = YOLOv12Neck([128, 256, 1024], [128, 256, 512])\n",
    "        self.head = YOLOv12Head([128, 256, 512], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get features from backbone\n",
    "        backbone_features = self.backbone(x)\n",
    "        \n",
    "        # Process features through neck\n",
    "        neck_features = self.neck(backbone_features)\n",
    "        \n",
    "        # Get detection outputs from head\n",
    "        outputs = self.head(neck_features)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training YOLOv12 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# We'll use Ultralytics YOLO as a base and customize it for YOLOv12\n",
    "# First, let's create a custom YOLOv12 configuration file\n",
    "\n",
    "with open('yolov12.yaml', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "# YOLOv12 configuration for Military Aircraft Detection\n",
    "\n",
    "# Parameters\n",
    "nc: 81  # number of classes (will be updated dynamically)\n",
    "depth_multiple: 1.0  # model depth multiple\n",
    "width_multiple: 1.0  # layer channel multiple\n",
    "\n",
    "# Anchors\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv12 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Conv, [32, 3, 1, 1]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [64, 3, 2, 1]],  # 1-P2/4\n",
    "   [-1, 1, ELAN, [64]],\n",
    "   [-1, 1, Conv, [128, 3, 2, 1]],  # 3-P3/8\n",
    "   [-1, 1, ELAN, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2, 1]],  # 5-P4/16\n",
    "   [-1, 1, ELAN, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2, 1]],  # 7-P5/32\n",
    "   [-1, 1, ELAN, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2, 1]],  # 9-P6/64\n",
    "   [-1, 1, ELAN, [1024]],\n",
    "   [-1, 1, SPPF, [1024, 5]],  # 11\n",
    "  ]\n",
    "\n",
    "# YOLOv12 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 8], 1, Concat, [1]],  # cat backbone P5\n",
    "   [-1, 1, ELAN, [512]],  # 15\n",
    "   \n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 1, ELAN, [256]],  # 19\n",
    "   \n",
    "   [-1, 1, Conv, [128, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 1, ELAN, [128]],  # 23 (P3/8-small)\n",
    "   \n",
    "   [-1, 1, Conv, [128, 3, 2]],\n",
    "   [[-1, 19], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 1, ELAN, [256]],  # 26 (P4/16-medium)\n",
    "   \n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 15], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 1, ELAN, [512]],  # 29 (P5/32-large)\n",
    "   \n",
    "   [[23, 26, 29], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n",
    "\"\"\")\n",
    "\n",
    "# Update the number of classes in the YAML file\n",
    "import yaml\n",
    "with open('yolov12.yaml', 'r') as f:\n",
    "    yolo_config = yaml.safe_load(f)\n",
    "\n",
    "yolo_config['nc'] = len(class_to_idx)\n",
    "\n",
    "with open('yolov12.yaml', 'w') as f:\n",
    "    yaml.dump(yolo_config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train YOLOv12 model using Ultralytics YOLO\n",
    "!yolo task=detect mode=train model=yolov12.yaml data=yolo_dataset/dataset.yaml epochs=50 imgsz=640 batch=16 name=yolov12_military_aircraft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the trained model\n",
    "!yolo task=detect mode=val model=runs/detect/yolov12_military_aircraft/weights/best.pt data=yolo_dataset/dataset.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model_path, image_paths, conf_threshold=0.25):\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        if i >= 9:  # Limit to 9 images\n",
    "            break\n",
    "            \n",
    "        results = model(img_path, conf=conf_threshold)[0]\n",
    "        \n",
    "        # Get the image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Plot in a 3x3 grid\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for box in results.boxes.xyxy:\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                             fill=False, edgecolor='red', linewidth=2))\n",
    "        \n",
    "        # Add class labels and confidence scores\n",
    "        for box, cls, conf in zip(results.boxes.xyxy, results.boxes.cls, results.boxes.conf):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            class_name = results.names[int(cls.item())]\n",
    "            confidence = conf.item()\n",
    "            plt.text(x1, y1-10, f\"{class_name}: {confidence:.2f}\", color='red', fontsize=12, \n",
    "                     bbox=dict(facecolor='white', alpha=0.7))\n",
    "        \n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get some validation images\n",
    "val_images = glob.glob('yolo_dataset/images/val/*.jpg')[:9]\n",
    "visualize_predictions('runs/detect/yolov12_military_aircraft/weights/best.pt', val_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference with YOLOv12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to perform inference on new images\n",
    "def detect_aircraft(model_path, image_path, conf_threshold=0.25):\n",
    "    model = YOLO(model_path)\n",
    "    results = model(image_path, conf=conf_threshold)[0]\n",
    "    \n",
    "    # Get the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for box, cls, conf in zip(results.boxes.xyxy, results.boxes.cls, results.boxes.conf):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        class_name = results.names[int(cls.item())]\n",
    "        confidence = conf.item()\n",
    "        \n",
    "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                         fill=False, edgecolor='red', linewidth=2))\n",
    "        plt.text(x1, y1-10, f\"{class_name}: {confidence:.2f}\", color='red', fontsize=12, \n",
    "                 bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.title(f\"Aircraft Detection: {os.path.basename(image_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection results\n",
    "    print(f\"Detected {len(results.boxes)} aircraft:\")\n",
    "    for i, (cls, conf) in enumerate(zip(results.boxes.cls, results.boxes.conf)):\n",
    "        class_name = results.names[int(cls.item())]\n",
    "        confidence = conf.item()\n",
    "        print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "# Try inference on a sample image\n",
    "sample_image = val_images[0] if val_images else None\n",
    "if sample_image:\n",
    "    detect_aircraft('runs/detect/yolov12_military_aircraft/weights/best.pt', sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export the model to different formats\n",
    "!yolo export model=runs/detect/yolov12_military_aircraft/weights/best.pt format=onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Usage Instructions\n",
    "\n",
    "This notebook has implemented YOLOv12 for the Military Aircraft Detection Dataset. The implementation includes:\n",
    "\n",
    "1. Data preprocessing and conversion to YOLO format\n",
    "2. YOLOv12 model architecture implementation\n",
    "3. Model training and evaluation\n",
    "4. Inference examples\n",
    "5. Model export for deployment\n",
    "\n",
    "### How to Use the Model\n",
    "\n",
    "To use the trained model for inference on new images:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO('runs/detect/yolov12_military_aircraft/weights/best.pt')\n",
    "\n",
    "# Perform inference\n",
    "results = model('path/to/image.jpg')\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding boxes outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "```\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "The model's performance metrics are available in the evaluation section. The key metrics include:\n",
    "\n",
    "- mAP (mean Average Precision)\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "### Further Improvements\n",
    "\n",
    "To further improve the model performance, consider:\n",
    "\n",
    "1. Increasing the training epochs\n",
    "2. Applying more data augmentation techniques\n",
    "3. Fine-tuning hyperparameters\n",
    "4. Using transfer learning from a pre-trained model\n",
    "5. Implementing ensemble methods with multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
